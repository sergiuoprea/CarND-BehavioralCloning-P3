{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "#Loading our data\n",
    "import csv as csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#Keras imports\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Lambda, Conv2D, MaxPooling2D, Activation\n",
    "from keras import backend as K\n",
    "\n",
    "#Matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "In order to load our data we firstly read the driving_log.csv file which contains the paths to the left, center and right cameras and also info about steering angle, throttle, break and speed. \n",
    "\n",
    "We will want to automate the loading process in order to easily load our own data which is stored in different directories. For that we used several libraries such as csv (read csv files) and cv2 (read images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the files\n",
    "\n",
    "#Reading the csv file\n",
    "def readCSV(in_path, in_dir_path):\n",
    "    lines = []\n",
    "    with open(in_path) as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader, None) #Skip header\n",
    "        for line in reader:\n",
    "            #Correct images paths\n",
    "            line[0] = in_dir_path + line[0].split('/')[-1]\n",
    "            line[1] = in_dir_path + line[1].split('/')[-1]\n",
    "            line[2] = in_dir_path + line[2].split('/')[-1]\n",
    "            \n",
    "            lines.append(line)\n",
    "            \n",
    "    return lines\n",
    "\n",
    "def readData_Basic(in_lines):\n",
    "    images = []\n",
    "    measurements = []\n",
    "    \n",
    "    for line in in_lines:\n",
    "        image = cv2.imread(line[0])\n",
    "        images.append(image)\n",
    "        \n",
    "        measurement = float(line[3])\n",
    "        measurements.append(measurement)\n",
    "    \n",
    "    return np.array(images), np.array(measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "Data loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "imgs_path = './data/IMG/'\n",
    "csv_path = './data/driving_log.csv'\n",
    "\n",
    "csv_lines = readCSV(csv_path, imgs_path)\n",
    "\n",
    "X_train, y_train = readData_Basic(csv_lines)\n",
    "\n",
    "print(type(y_train))\n",
    "\n",
    "print('Data loaded successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "print(type(y_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "To grayscale?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(in_image):\n",
    "    return cv2.cvtColor(in_image, cv2.COLOR_BGR2YUV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic neural network using Keras\n",
    "\n",
    "We will implement a basic neural network in order to verify that everything is working before implementing a more complex model. This network will just going to be a flattened image connected to a single output node. This single output node will predict the stearing angle, thus converting this model into a regression network. In contrast with a classification network, we may apply a softmax activation function to the output layer. Nevertheless in this case we will not use an activation function. We will directly predict the steering measurement. \n",
    "\n",
    "For this basic implementation we will use Keras as a library which works with tensorflow as backend. This will simplify our implementation and will be great for prototyping. Let's go ahead!\n",
    "\n",
    "Improvement 1: In order to improve our model we need to preprocess our input data. For that we will add two preprocessing steps: normalization and  mean centering the data. We will add a lambda layer to our model. After doing this, we can decrease the training epochs a lot. We will fix the number of epochs in 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6428 samples, validate on 1608 samples\n",
      "Epoch 1/2\n",
      "6428/6428 [==============================] - 2s 384us/step - loss: 1.8318 - val_loss: 0.7415\n",
      "Epoch 2/2\n",
      "6428/6428 [==============================] - 2s 370us/step - loss: 2.9713 - val_loss: 2.7356\n"
     ]
    }
   ],
   "source": [
    "#Model definition\n",
    "\n",
    "model = Sequential()\n",
    "#Lambda layer for normalizing our data. In order to mean center the data, we will\n",
    "#need to substract -0.5 (shifting the model down) to the normalized data. \n",
    "model.add(Lambda(lambda x: x / 255.0 - 0.5, input_shape=(160,320,3)))\n",
    "model.add(Flatten(input_shape=(160,320,3)))\n",
    "model.add(Dense(1))\n",
    "\n",
    "#Model compilation\n",
    "#For the loss function we will use Mean Squared Error (MSE). We will minimize the \n",
    "#error between the steering measurement which the network predicts and the ground \n",
    "#truth steering measurements provided by the dataset\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "#we also shuffle the data and split off 20% of the data to use for a validation set\n",
    "model.fit(X_train, y_train, validation_split=0.2, shuffle=True, epochs= 2)\n",
    "\n",
    "#Keras by default will run 10 epochs. Nevertheless with 10 epochs we will \n",
    "#overfit the training data. For that reason we will only perform 6 epochs\n",
    "model.save('basic_model.h5')\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will try to autonomous drive the car with this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying a more complex network such as LeNet-5 architecture\n",
    "\n",
    "We will implement LeNet-5 architecture using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6428 samples, validate on 1608 samples\n",
      "Epoch 1/2\n",
      "6428/6428 [==============================] - 4s 676us/step - loss: 1.5497 - val_loss: 0.0136\n",
      "Epoch 2/2\n",
      "6428/6428 [==============================] - 4s 629us/step - loss: 0.0113 - val_loss: 0.0120\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "#Lambda layer for normalizing our data. In order to mean center the data, we will\n",
    "#need to substract -0.5 (shifting the model down) to the normalized data. \n",
    "model1.add(Lambda(lambda x: x / 255.0 - 0.5, input_shape=X_train.shape[1:]))\n",
    "\n",
    "#First set of CONV => RELU => POOL\n",
    "model1.add(Conv2D(3, (5, 5), input_shape= X_train.shape[1:]))\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(MaxPooling2D(pool_size= (2, 2), strides= (2, 2)))\n",
    "\n",
    "#Second set of CONV => RELU => POOL\n",
    "model1.add(Conv2D(6, (5, 5)))\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(MaxPooling2D(pool_size= (2, 2), strides= (2, 2)))\n",
    "\n",
    "#Setting the FCs layers\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(120))\n",
    "model1.add(Dense(84))\n",
    "\n",
    "#Output layer\n",
    "model1.add(Dense(1))\n",
    "\n",
    "#Model compilation\n",
    "#For the loss function we will use Mean Squared Error (MSE). We will minimize the \n",
    "#error between the steering measurement which the network predicts and the ground \n",
    "#truth steering measurements provided by the dataset\n",
    "model1.compile(loss='mse', optimizer='adam')\n",
    "#we also shuffle the data and split off 20% of the data to use for a validation set\n",
    "model1.fit(X_train, y_train, validation_split=0.2, shuffle=True, epochs= 2)\n",
    "\n",
    "#Keras by default will run 10 epochs. Nevertheless with 10 epochs we will \n",
    "#overfit the training data. For that reason we will only perform 6 epochs\n",
    "model1.save('lenet_model.h5')\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation\n",
    "\n",
    "In order to increase the number of data we have driven the car in the opposite direction along the routes. At the same time we can flip the images and also taking the opposite sign of steering measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataAugmentation(in_image, in_measurement):\n",
    "    image_flipped = np.fliplr(image)\n",
    "    measurement_flipped = -measurement\n",
    "    \n",
    "    return image_flipped, measurement_flipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Multiple Cameras\n",
    "\n",
    "Up to this point we only used the center camera. But, using side cameras should be a great decision because we will have three times more data. And also, using these images we will teach the network how to steer back to the center if the vehicle starts drifting off to the side. \n",
    "\n",
    "The simulator captures images from three cameras mounted on the car: a center, right and left camera. That’s because of the issue of recovering from being off-center. In the simulator, you can weave all over the road and turn recording on and off to record recovery driving. In a real car, however, that’s not really possible. At least not legally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData_Advanced(in_lines, correction=0.25):\n",
    "    images = []\n",
    "    measurements = []\n",
    "    \n",
    "    for line in in_lines:\n",
    "        #Images\n",
    "        center_img = cv2.imread(line[0])\n",
    "        left_img = cv2.imread(line[1])\n",
    "        right_img = cv2.imread(line[2])\n",
    "        \n",
    "        center_img = preprocess_image(center_img)\n",
    "        left_img = preprocess_image(left_img)\n",
    "        right_img = preprocess_image(right_img)\n",
    "        \n",
    "        images.append(center_img)\n",
    "        images.append(left_img)\n",
    "        images.append(right_img)\n",
    "        \n",
    "        #Measurements\n",
    "        center_steer = float(line[3])\n",
    "        left_steer = center_steer + correction\n",
    "        right_steer = center_steer - correction\n",
    "\n",
    "        measurements.append(center_steer)\n",
    "        measurements.append(left_steer)\n",
    "        measurements.append(right_steer)\n",
    "    \n",
    "    return np.array(images), np.array(measurements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nVidia Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NvidiaArchitecture():\n",
    "    model = Sequential()\n",
    "    #Preprocessing\n",
    "    model.add(Lambda(lambda x: x / 255.0 - 0.5, input_shape=X_train.shape[1:]))\n",
    "    model.add(Cropping2D(cropping=((50,20), (0,0))))\n",
    "    \n",
    "    model.add(Conv2D(24,(5,5), subsample=(2,2), activation='relu'))\n",
    "    model.add(Conv2D(36,(5,5), subsample=(2,2), activation='relu'))\n",
    "    model.add(Conv2D(48,(5,5), subsample=(2,2), activation='relu'))\n",
    "    model.add(Conv2D(64,(3,3), activation='relu'))\n",
    "    model.add(Conv2D(64,(3,3), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100))\n",
    "    model.add(Dense(50))\n",
    "    model.add(Dense(10))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
