{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "#Loading our data\n",
    "import csv as csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#Keras imports\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Lambda, Conv2D, MaxPooling2D, Activation\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "In order to load our data we firstly read the driving_log.csv file which contains the paths to the left, center and right cameras and also info about steering angle, throttle, break and speed. \n",
    "\n",
    "We will want to automate the loading process in order to easily load our own data which is stored in different directories. For that we used several libraries such as csv (read csv files) and cv2 (read images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the files\n",
    "\n",
    "#Reading the csv file\n",
    "def readCSV(in_path):\n",
    "    lines = []\n",
    "    with open(in_path) as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader, None) #Skip header\n",
    "        for line in reader:\n",
    "            lines.append(line)\n",
    "            \n",
    "    return lines\n",
    "\n",
    "def readData(in_dir, in_lines):\n",
    "    images = []\n",
    "    measurements = []\n",
    "    for line in in_lines:\n",
    "        source_path = line[0]\n",
    "        filename = source_path.split('/')[-1]\n",
    "        path = in_dir + filename\n",
    "        image = cv2.imread(path)\n",
    "        images.append(image)\n",
    "        measurement = float(line[3])\n",
    "        measurements.append(measurement)\n",
    "    \n",
    "    return np.array(images), np.array(measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "Data loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "imgs_path = './data/IMG/'\n",
    "csv_path = './data/driving_log.csv'\n",
    "\n",
    "X_train, y_train = readData(imgs_path, readCSV(csv_path))\n",
    "\n",
    "print(type(y_train))\n",
    "\n",
    "print('Data loaded successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "print(type(y_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic neural network using Keras\n",
    "\n",
    "We will implement a basic neural network in order to verify that everything is working before implementing a more complex model. This network will just going to be a flattened image connected to a single output node. This single output node will predict the stearing angle, thus converting this model into a regression network. In contrast with a classification network, we may apply a softmax activation function to the output layer. Nevertheless in this case we will not use an activation function. We will directly predict the steering measurement. \n",
    "\n",
    "For this basic implementation we will use Keras as a library which works with tensorflow as backend. This will simplify our implementation and will be great for prototyping. Let's go ahead!\n",
    "\n",
    "Improvement 1: In order to improve our model we need to preprocess our input data. For that we will add two preprocessing steps: normalization and  mean centering the data. We will add a lambda layer to our model. After doing this, we can decrease the training epochs a lot. We will fix the number of epochs in 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6428 samples, validate on 1608 samples\n",
      "Epoch 1/2\n",
      "6428/6428 [==============================] - 2s 384us/step - loss: 1.8318 - val_loss: 0.7415\n",
      "Epoch 2/2\n",
      "6428/6428 [==============================] - 2s 370us/step - loss: 2.9713 - val_loss: 2.7356\n"
     ]
    }
   ],
   "source": [
    "#Model definition\n",
    "\n",
    "model = Sequential()\n",
    "#Lambda layer for normalizing our data. In order to mean center the data, we will\n",
    "#need to substract -0.5 (shifting the model down) to the normalized data. \n",
    "model.add(Lambda(lambda x: x / 255.0 - 0.5, input_shape=(160,320,3)))\n",
    "model.add(Flatten(input_shape=(160,320,3)))\n",
    "model.add(Dense(1))\n",
    "\n",
    "#Model compilation\n",
    "#For the loss function we will use Mean Squared Error (MSE). We will minimize the \n",
    "#error between the steering measurement which the network predicts and the ground \n",
    "#truth steering measurements provided by the dataset\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "#we also shuffle the data and split off 20% of the data to use for a validation set\n",
    "model.fit(X_train, y_train, validation_split=0.2, shuffle=True, epochs= 2)\n",
    "\n",
    "#Keras by default will run 10 epochs. Nevertheless with 10 epochs we will \n",
    "#overfit the training data. For that reason we will only perform 6 epochs\n",
    "model.save('basic_model.h5')\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will try to autonomous drive the car with this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying a more complex network such as LeNet-5 architecture\n",
    "\n",
    "We will implement LeNet-5 architecture using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6428 samples, validate on 1608 samples\n",
      "Epoch 1/2\n",
      "6428/6428 [==============================] - 4s 672us/step - loss: 1.0497 - val_loss: 0.0153\n",
      "Epoch 2/2\n",
      "6428/6428 [==============================] - 4s 643us/step - loss: 0.0124 - val_loss: 0.0131\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#Lambda layer for normalizing our data. In order to mean center the data, we will\n",
    "#need to substract -0.5 (shifting the model down) to the normalized data. \n",
    "model.add(Lambda(lambda x: x / 255.0 - 0.5, input_shape=input_shape))\n",
    "\n",
    "#First set of CONV => RELU => POOL\n",
    "model.add(Conv2D(3, (5, 5), input_shape= X_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size= (2, 2), strides= (2, 2)))\n",
    "\n",
    "#Second set of CONV => RELU => POOL\n",
    "model.add(Conv2D(6, (5, 5)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size= (2, 2), strides= (2, 2)))\n",
    "\n",
    "#Setting the FCs layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(120))\n",
    "model.add(Dense(84))\n",
    "\n",
    "#Output layer\n",
    "model.add(Dense(1))\n",
    "\n",
    "#Model compilation\n",
    "#For the loss function we will use Mean Squared Error (MSE). We will minimize the \n",
    "#error between the steering measurement which the network predicts and the ground \n",
    "#truth steering measurements provided by the dataset\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "#we also shuffle the data and split off 20% of the data to use for a validation set\n",
    "model.fit(X_train, y_train, validation_split=0.2, shuffle=True, epochs= 2)\n",
    "\n",
    "#Keras by default will run 10 epochs. Nevertheless with 10 epochs we will \n",
    "#overfit the training data. For that reason we will only perform 6 epochs\n",
    "model.save('lenet_model.h5')\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
